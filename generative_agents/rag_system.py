import json
import os
import pickle
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import gradio as gr
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from faiss import IndexFlatL2
from langchain_community.docstore.in_memory import InMemoryDocstore
import threading
import time
from concurrent.futures import ThreadPoolExecutor

# ç¦ç”¨HuggingFaceç¬¦å·é“¾æ¥è­¦å‘Š
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

class ChineseEmbeddings(Embeddings):
    """åŸºäºOllamaçš„ä¸­æ–‡åµŒå…¥æ¨¡å‹åŒ…è£…å™¨"""
    
    def __init__(self, model_name: str = "bge-m3:latest", base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–ä¸­æ–‡åµŒå…¥æ¨¡å‹
        ä½¿ç”¨Ollamaçš„bge-m3æ¨¡å‹ï¼Œå¯¹ä¸­æ–‡æ”¯æŒå¾ˆå¥½
        """
        self.model = OllamaEmbeddings(
            model=model_name,
            base_url=base_url
        )
        print(f"å·²åŠ è½½Ollamaä¸­æ–‡åµŒå…¥æ¨¡å‹: {model_name}")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨"""
        return self.model.embed_documents(texts)
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        return self.model.embed_query(text)

class JSONRAGSystem:
    """åŸºäºJSONæ–‡ä»¶çš„RAGé—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, result_dir: str = "results/compressed", cache_dir: str = "vector_cache"):
        # è§„èŒƒåŒ–ç»“æœç›®å½•ä¸ºç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„ï¼Œé¿å…å·¥ä½œç›®å½•å˜åŒ–å¯¼è‡´æ‰¾ä¸åˆ°é¡¹ç›®
        base_dir = Path(__file__).parent
        provided_path = Path(result_dir)
        self.result_dir = (provided_path if provided_path.is_absolute() else (base_dir / provided_path)).resolve()
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºè§£æåçš„é¡¹ç›®æ ¹ç›®å½•
        print(f"é¡¹ç›®æ ¹ç›®å½•: {self.result_dir}")
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedder = ChineseEmbeddings()
        self.embedder_model_name = "bge-m3:latest"
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " "]
        )
        
        # åˆå§‹åŒ–LLM
        self.llm = self._init_llm()
        
        # å‘é‡å­˜å‚¨
        self.vectorstore = None
        self.conversation_store = None
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
    
    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        try:
            return OllamaLLM(
                model="qwen3:4b-q4_K_M",
                base_url="http://localhost:11434",
                temperature=0.7
            )
        except Exception as e:
            print(f"âŒ LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”å·²ä¸‹è½½qwen3:4b-q4_K_Mæ¨¡å‹")
            return None
    
    def _get_json_hash(self, file_path: str) -> str:
        """ç”ŸæˆJSONæ–‡ä»¶çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        content = f"{file_path}_{self.embedder_model_name}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_project_hash(self, project_path: str) -> str:
        """ç”Ÿæˆé¡¹ç›®çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        content = f"{project_path}_{self.embedder_model_name}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _check_vectorized(self, project_path: str) -> bool:
        """æ£€æŸ¥é¡¹ç›®æ˜¯å¦å·²å‘é‡åŒ–ï¼ˆå…¼å®¹ä¸åŒå“ˆå¸Œ/è·¯å¾„ç”Ÿæˆæ–¹å¼ï¼‰"""
        project_dir = Path(project_path)
        if not project_dir.exists():
            return False

        try:
            # ç›´æ¥é€šè¿‡æ¨¡å¼åŒ¹é…æ£€æµ‹æ˜¯å¦å­˜åœ¨å‘é‡ç›®å½•å’Œå¯¹åº”çš„metadataæ–‡ä»¶
            vector_dirs = list(project_dir.glob("*_vector"))
            metadata_files = list(project_dir.glob("*_metadata.pkl"))

            if not vector_dirs or not metadata_files:
                return False

            # è¦æ±‚æœ‰åŒå‰ç¼€çš„ä¸€å¯¹ï¼ˆæ›´ç¨³å¥ï¼‰
            vector_prefixes = {p.name[:-len("_vector")] for p in vector_dirs if p.is_dir()}
            metadata_prefixes = {p.name[:-len("_metadata.pkl")] for p in metadata_files}

            return len(vector_prefixes.intersection(metadata_prefixes)) > 0
        except Exception:
            return False
    
    def _save_vectorized(self, file_path: str, vectorstore, documents: List[Document], metadata: Dict):
        """ä¿å­˜å‘é‡åŒ–æ•°æ®"""
        file_hash = self._get_json_hash(file_path)
        
        # ä¿å­˜å‘é‡å­˜å‚¨
        vector_file = self.cache_dir / f"{file_hash}_vector.pkl"
        with open(vector_file, 'wb') as f:
            pickle.dump(vectorstore, f)
        
        # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
        metadata_file = self.cache_dir / f"{file_hash}_metadata.pkl"
        with open(metadata_file, 'wb') as f:
            pickle.dump({
                'documents': documents,
                'metadata': metadata
            }, f)
        
        print(f"âœ… å·²ä¿å­˜å‘é‡åŒ–æ•°æ®: {file_path}")
    
    def _save_project_vectorized(self, project_dir: Path, vectorstore, documents: List[Document], metadata: Dict):
        """ä¿å­˜é¡¹ç›®å‘é‡åŒ–æ•°æ®åˆ°é¡¹ç›®ç›®å½•"""
        project_hash = self._get_project_hash(str(project_dir))
        
        # åœ¨é¡¹ç›®ç›®å½•ä¸‹åˆ›å»ºå‘é‡åŒ–æ–‡ä»¶
        vector_file = project_dir / f"{project_hash}_vector.pkl"
        metadata_file = project_dir / f"{project_hash}_metadata.pkl"
        
        try:
            # ä¿å­˜å‘é‡å­˜å‚¨ - ä½¿ç”¨FAISSçš„saveæ–¹æ³•
            vectorstore.save_local(str(vector_file.with_suffix('')))
            
            # ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
            with open(metadata_file, 'wb') as f:
                pickle.dump({
                    'documents': documents,
                    'metadata': metadata
                }, f)
            
            print(f"âœ… å·²ä¿å­˜é¡¹ç›®å‘é‡åŒ–æ•°æ®: {project_dir.name}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å‘é‡åŒ–æ•°æ®å¤±è´¥: {e}")
            # å¦‚æœFAISSä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜æ–‡æ¡£å’Œå…ƒæ•°æ®
            with open(metadata_file, 'wb') as f:
                pickle.dump({
                    'documents': documents,
                    'metadata': metadata
                }, f)
    
    def _load_vectorized(self, file_path: str) -> Tuple[FAISS, List[Document], Dict]:
        """åŠ è½½å‘é‡åŒ–æ•°æ®"""
        file_hash = self._get_json_hash(file_path)
        
        vector_file = self.cache_dir / f"{file_hash}_vector.pkl"
        metadata_file = self.cache_dir / f"{file_hash}_metadata.pkl"
        
        with open(vector_file, 'rb') as f:
            vectorstore = pickle.load(f)
        
        with open(metadata_file, 'rb') as f:
            data = pickle.load(f)
            documents = data['documents']
            metadata = data['metadata']
        
        print(f"âœ… å·²ä»ç¼“å­˜åŠ è½½: {file_path}")
        return vectorstore, documents, metadata
    
    def _extract_movement_json(self, json_file: Path) -> List[Document]:
        """ä»movement.jsonæ–‡ä»¶ä¸­æå–å†…å®¹å¹¶è½¬æ¢ä¸ºæ–‡æ¡£"""
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            documents = []
            
            # å¤„ç†movement.jsonçš„ç‰¹å®šç»“æ„
            if isinstance(data, list):
                # å¤„ç†ç§»åŠ¨è®°å½•åˆ—è¡¨
                for i, movement in enumerate(data):
                    if isinstance(movement, dict):
                        content = f"ç§»åŠ¨è®°å½• {i+1}:\n"
                        content += f"æ—¶é—´: {movement.get('timestamp', 'æœªçŸ¥')}\n"
                        content += f"è§’è‰²: {movement.get('character', 'æœªçŸ¥')}\n"
                        content += f"ä½ç½®: {movement.get('location', 'æœªçŸ¥')}\n"
                        content += f"åŠ¨ä½œ: {movement.get('action', 'æœªçŸ¥')}\n"
                        if 'details' in movement:
                            content += f"è¯¦æƒ…: {json.dumps(movement['details'], ensure_ascii=False, indent=2)}"
                        
                        doc = Document(
                            page_content=content,
                            metadata={
                                'source': str(json_file),
                                'type': 'movement',
                                'index': i,
                                'file_name': json_file.name,
                                'file_path': str(json_file)
                            }
                        )
                        documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"âŒ å¤„ç†movement.jsonæ–‡ä»¶å¤±è´¥ {json_file}: {e}")
            return []
    
    def _extract_simulation_md(self, md_file: Path) -> List[Document]:
        """ä»simulation.mdæ–‡ä»¶ä¸­æå–å†…å®¹å¹¶è½¬æ¢ä¸ºæ–‡æ¡£"""
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŒ‰æ®µè½åˆ†å‰²markdownå†…å®¹
            paragraphs = content.split('\n\n')
            documents = []
            
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():
                    doc = Document(
                        page_content=paragraph.strip(),
                        metadata={
                            'source': str(md_file),
                            'type': 'simulation',
                            'paragraph': i,
                            'file_name': md_file.name,
                            'file_path': str(md_file)
                        }
                    )
                    documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"âŒ å¤„ç†simulation.mdæ–‡ä»¶å¤±è´¥ {md_file}: {e}")
            return []
    
    def _process_project_files(self, project_dir: Path) -> bool:
        """å¤„ç†é¡¹ç›®æ–‡ä»¶å¤¹ä¸­çš„movement.jsonå’Œsimulation.md"""
        try:
            movement_file = project_dir / "movement.json"
            simulation_file = project_dir / "simulation.md"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not movement_file.exists() and not simulation_file.exists():
                print(f"é¡¹ç›®æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°movement.jsonæˆ–simulation.md: {project_dir.name}")
                return False
            
            # æ£€æŸ¥æ˜¯å¦å·²å‘é‡åŒ–
            project_hash = self._get_project_hash(str(project_dir))
            if self._check_vectorized(project_hash):
                print(f"è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®: {project_dir.name}")
                return True
            
            print(f"å¤„ç†é¡¹ç›®: {project_dir.name}")
            
            # åˆå¹¶å¤„ç†movement.jsonå’Œsimulation.md
            combined_content = ""
            file_info = []
            
            # å¤„ç†movement.json
            if movement_file.exists():
                print(f"å¤„ç†movement.json...")
                with open(movement_file, 'r', encoding='utf-8') as f:
                    movement_data = json.load(f)
                
                # æå–å…³é”®ä¿¡æ¯
                movement_text = f"=== MOVEMENT DATA ===\n"
                movement_text += f"å¼€å§‹æ—¶é—´: {movement_data.get('start_datetime', 'æœªçŸ¥')}\n"
                movement_text += f"æ­¥é•¿: {movement_data.get('stride', 'æœªçŸ¥')}\n"
                movement_text += f"æ¯ç§’æ­¥æ•°: {movement_data.get('sec_per_step', 'æœªçŸ¥')}\n\n"
                
                # æ·»åŠ åˆå§‹ä½ç½®ä¿¡æ¯
                if 'persona_init_pos' in movement_data:
                    movement_text += "åˆå§‹ä½ç½®:\n"
                    for persona, pos in movement_data['persona_init_pos'].items():
                        movement_text += f"  {persona}: {pos}\n"
                
                # æ·»åŠ ç§»åŠ¨è½¨è¿¹æ•°æ®ï¼ˆé‡‡æ ·æ˜¾ç¤ºï¼‰
                if 'trajectory' in movement_data:
                    trajectory = movement_data['trajectory']
                    movement_text += f"\nç§»åŠ¨è½¨è¿¹æ•°æ® (å…±{len(trajectory)}æ¡è®°å½•):\n"
                    # åªæ˜¾ç¤ºå‰10æ¡å’Œæœ€å10æ¡è®°å½•
                    for i, record in enumerate(trajectory[:10]):
                        movement_text += f"  {i+1}: {record}\n"
                    if len(trajectory) > 20:
                        movement_text += f"  ... (çœç•¥{len(trajectory)-20}æ¡è®°å½•) ...\n"
                    for i, record in enumerate(trajectory[-10:], len(trajectory)-9):
                        movement_text += f"  {i}: {record}\n"
                
                combined_content += movement_text + "\n\n"
                file_info.append("movement.json")
                print(f"movement.json: å·²æå–å…³é”®ä¿¡æ¯")
            
            # å¤„ç†simulation.md
            if simulation_file.exists():
                print(f"å¤„ç†simulation.md...")
                with open(simulation_file, 'r', encoding='utf-8') as f:
                    simulation_content = f.read()
                
                combined_content += f"=== SIMULATION DATA ===\n{simulation_content}\n\n"
                file_info.append("simulation.md")
                print(f"simulation.md: å·²è¯»å–å†…å®¹")
            
            if not combined_content.strip():
                print(f"é¡¹ç›®å†…å®¹ä¸ºç©º: {project_dir.name}")
                return False
            
            # åˆ›å»ºå•ä¸ªæ–‡æ¡£
            doc = Document(
                page_content=combined_content,
                metadata={
                    'source': str(project_dir),
                    'project_name': project_dir.name,
                    'files': file_info,
                    'file_name': f"{project_dir.name}_combined",
                    'file_path': str(project_dir)
                }
            )
            
            # åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_documents([doc])
            
            if not chunks:
                print(f"åˆ†å‰²åæ— æœ‰æ•ˆå†…å®¹: {project_dir.name}")
                return False
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vectorstore = FAISS.from_documents(chunks, self.embedder)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'project_name': project_dir.name,
                'project_path': str(project_dir),
                'movement_file': str(movement_file) if movement_file.exists() else None,
                'simulation_file': str(simulation_file) if simulation_file.exists() else None,
                'total_chunks': len(chunks),
                'processed_time': time.time()
            }
            
            # ä¿å­˜å‘é‡åŒ–æ•°æ®åˆ°é¡¹ç›®ç›®å½•
            self._save_project_vectorized(project_dir, vectorstore, chunks, metadata)
            
            print(f"é¡¹ç›®å¤„ç†å®Œæˆ: {project_dir.name} ({len(chunks)} ä¸ªå—)")
            return True
            
        except Exception as e:
            print(f"å¤„ç†é¡¹ç›®å¤±è´¥ {project_dir.name}: {e}")
            return False
    
    def extract_and_vectorize(self, project_name: str) -> bool:
        """å¤„ç†æŒ‡å®šé¡¹ç›®çš„movement.jsonå’Œsimulation.mdæ–‡ä»¶"""
        project_dir = self.result_dir / project_name
        
        if not project_dir.exists():
            print(f"âŒ é¡¹ç›®ç›®å½•ä¸å­˜åœ¨: {project_dir}")
            return False
        
        print(f"ğŸ” å¤„ç†é¡¹ç›®: {project_dir}")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        movement_file = project_dir / "movement.json"
        simulation_file = project_dir / "simulation.md"
        
        if not movement_file.exists() and not simulation_file.exists():
            print(f"âŒ é¡¹ç›®ç›®å½•ä¸­æœªæ‰¾åˆ°movement.jsonæˆ–simulation.md: {project_dir}")
            return False
        
        # å¤„ç†é¡¹ç›®æ–‡ä»¶
        return self._process_project_files(project_dir)
    
    def _load_all_vectorized(self) -> FAISS:
        """åŠ è½½æ‰€æœ‰å·²å‘é‡åŒ–çš„æ•°æ®"""
        if not self.result_dir.exists():
            return self._create_empty_vectorstore()
        
        vectorstores = []
        metadata_list = []
        loaded_projects = []
        
        # éå†æ‰€æœ‰é¡¹ç›®ç›®å½•ï¼ŒæŸ¥æ‰¾å‘é‡åŒ–æ–‡ä»¶
        for project_dir in self.result_dir.iterdir():
            if project_dir.is_dir():
                # æŸ¥æ‰¾é¡¹ç›®ç›®å½•ä¸­çš„å‘é‡åŒ–æ–‡ä»¶
                for vector_dir in project_dir.glob("*_vector"):
                    if vector_dir.is_dir():
                        try:
                            # ä½¿ç”¨FAISSçš„load_localæ–¹æ³•ï¼Œå…è®¸åŠ è½½pickleæ–‡ä»¶
                            vectorstore = FAISS.load_local(str(vector_dir), self.embedder, allow_dangerous_deserialization=True)
                            vectorstores.append(vectorstore)
                            loaded_projects.append(project_dir.name)
                            
                            # åŠ è½½å¯¹åº”çš„å…ƒæ•°æ®
                            metadata_file = project_dir / f"{vector_dir.stem}_metadata.pkl"
                            if metadata_file.exists():
                                with open(metadata_file, 'rb') as f:
                                    metadata = pickle.load(f)
                                    metadata_list.append(metadata)
                            
                        except Exception as e:
                            print(f"åŠ è½½å‘é‡æ–‡ä»¶å¤±è´¥ {vector_dir}: {e}")
                            continue
        
        if vectorstores:
            print(f"å·²åŠ è½½ {len(loaded_projects)} ä¸ªé¡¹ç›®çš„å‘é‡åŒ–æ•°æ®: {', '.join(loaded_projects)}")
        
        if not vectorstores:
            return self._create_empty_vectorstore()
        
        # åˆå¹¶æ‰€æœ‰å‘é‡å­˜å‚¨
        if len(vectorstores) == 1:
            return vectorstores[0]
        else:
            combined_store = vectorstores[0]
            for store in vectorstores[1:]:
                combined_store.merge_from(store)
            return combined_store
    
    def _create_empty_vectorstore(self) -> FAISS:
        """åˆ›å»ºç©ºçš„å‘é‡å­˜å‚¨"""
        embed_dims = len(self.embedder.embed_query("test"))
        return FAISS(
            embedding_function=self.embedder,
            index=IndexFlatL2(embed_dims),
            docstore=InMemoryDocstore(),
            index_to_docstore_id={},
            normalize_L2=False
        )
    
    #è¿™é‡Œæ¯æ¬¡æé—®åŠ è½½20ä¸ªå—
    def _retrieve_documents(self, query: str, k: int = 20) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.vectorstore:
            return []
        
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _format_context(self, docs: List[Document]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
        if not docs:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        context = "ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š\n\n"
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
            context += f"æ–‡æ¡£ {i} (æ¥æº: {source}):\n"
            context += f"{doc.page_content}\n\n"
        
        return context
    
    def query(self, question: str):
        """æŸ¥è¯¢é—®ç­” - æµå¼è¾“å‡º"""
        if not self.llm:
            yield "âŒ LLMæœªåˆå§‹åŒ–ï¼Œè¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ"
            return
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self._retrieve_documents(question)
        context = self._format_context(docs)
        
        # æ„å»ºæç¤º
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. åŸºäºæ–‡æ¡£çš„å®é™…å†…å®¹è¿›è¡Œå›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€æœ‰æ¡ç†
4. å¦‚æœæ¶‰åŠå¤šä¸ªæ–‡æ¡£ï¼Œè¯·æŒ‡å‡ºä¿¡æ¯æ¥æº

è¯·å¼€å§‹å›ç­”ï¼š"""
        
        try:
            # ä½¿ç”¨æµå¼è¾“å‡º
            for chunk in self.llm.stream(prompt):
                yield chunk
        except Exception as e:
            yield f"âŒ ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}"
    
    def get_available_projects(self) -> List[str]:
        """è·å–å¯ç”¨çš„é¡¹ç›®åˆ—è¡¨ï¼ˆåŒ…å«movement.jsonæˆ–simulation.mdçš„é¡¹ç›®ï¼‰"""
        if not self.result_dir.exists():
            return []
        
        projects = []
        for item in self.result_dir.iterdir():
            if item.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«movement.jsonæˆ–simulation.md
                movement_file = item / "movement.json"
                simulation_file = item / "simulation.md"
                
                if movement_file.exists() or simulation_file.exists():
                    projects.append(item.name)
        
        return sorted(projects)
    
    def get_project_stats(self, project_name: str) -> Dict[str, Any]:
        """è·å–é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯"""
        project_dir = self.result_dir / project_name
        if not project_dir.exists():
            return {"error": "é¡¹ç›®ä¸å­˜åœ¨"}
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        movement_file = project_dir / "movement.json"
        simulation_file = project_dir / "simulation.md"
        
        files_info = []
        if movement_file.exists():
            files_info.append("movement.json")
        if simulation_file.exists():
            files_info.append("simulation.md")
        
        # æ£€æŸ¥æ˜¯å¦å·²å‘é‡åŒ–
        is_vectorized = self._check_vectorized(str(project_dir))
        
        return {
            "total_files": len(files_info),
            "files": files_info,
            "vectorized": is_vectorized,
            "status": "å·²å‘é‡åŒ–" if is_vectorized else "æœªå‘é‡åŒ–"
        }
    
    def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # æ˜¾ç¤ºå¯ç”¨é¡¹ç›®
        available_projects = self.get_available_projects()
        if available_projects:
            print(f"æ‰¾åˆ° {len(available_projects)} ä¸ªå¯ç”¨é¡¹ç›®: {', '.join(available_projects)}")
        else:
            print("æœªæ‰¾åˆ°ä»»ä½•é¡¹ç›®")
        
        # åŠ è½½å‘é‡å­˜å‚¨
        self.vectorstore = self._load_all_vectorized()
        
        # åˆå§‹åŒ–å¯¹è¯å­˜å‚¨
        self.conversation_store = self._create_empty_vectorstore()
        
        print("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

def create_gradio_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    rag_system = JSONRAGSystem()
    rag_system.initialize_system()
    
    with gr.Blocks(
        title="JSONæ–‡æ¡£RAGé—®ç­”ç³»ç»Ÿ",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            width: 100%;
            max-width: none;
            margin: 0;
            padding: 0;
        }
        .header-section {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            margin-bottom: 20px;
            text-align: center;
            width: 100%;
        }
        .left-panel {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-right: 10px;
        }
        .right-panel {
            background: white;
            padding: 20px;
            border-radius: 10px;
            border: 1px solid #e9ecef;
        }
        .project-stats {
            background: #e3f2fd;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
        }
        .status-section {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
        }
        """
    ) as demo:
        
        # å¤´éƒ¨
        with gr.Column(elem_classes=["header-section"]):
            gr.HTML("""
                <h1 style="margin: 0; font-size: 2.5rem;">ğŸ“š JSONæ–‡æ¡£RAGé—®ç­”ç³»ç»Ÿ</h1>
                <p style="margin: 10px 0 0 0; font-size: 1.2rem; opacity: 0.9;">
                    åŸºäºJSONæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒä¸­æ–‡è¯­ä¹‰æœç´¢
                </p>
            """)
        
        # ä¸»ç•Œé¢ - ä¸¤åˆ—å¸ƒå±€
        with gr.Row():
            # å·¦ä¾§é¢æ¿ï¼Œscaleçš„æ„æ€æ˜¯è¿™ä¸ªåˆ—çš„å®½åº¦æ˜¯æ€»å®½åº¦çš„1/3
            with gr.Column(scale=1, elem_classes=["left-panel"]):
                # é¡¹ç›®ç®¡ç†
                gr.HTML('<h3>ğŸ¯ é¡¹ç›®ç®¡ç†</h3>')
                
                # é¡¹ç›®é€‰æ‹©
                project_dropdown = gr.Dropdown(
                    choices=rag_system.get_available_projects(),
                    label="é€‰æ‹©é¡¹ç›®",
                    interactive=True
                )
                
                # é¡¹ç›®ç»Ÿè®¡
                stats_display = gr.HTML(
                    value="<p>è¯·é€‰æ‹©ä¸€ä¸ªé¡¹ç›®æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯</p>",
                    label="é¡¹ç›®ç»Ÿè®¡"
                )
                
                # å¤„ç†æŒ‰é’®
                process_btn = gr.Button(
                    "ğŸš€ å¼€å§‹å‘é‡åŒ–å¤„ç†",
                    variant="primary",
                    size="lg"
                )
                
                # å¤„ç†çŠ¶æ€
                process_status = gr.Textbox(
                    label="å¤„ç†çŠ¶æ€",
                    interactive=False,
                    value="ç­‰å¾…å¤„ç†..."
                )
                
                # åˆ·æ–°æŒ‰é’®
                refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°é¡¹ç›®åˆ—è¡¨", variant="secondary")
                
                # ç³»ç»ŸçŠ¶æ€
                gr.HTML('<h3>ğŸ“Š ç³»ç»ŸçŠ¶æ€</h3>')
                system_status = gr.HTML(
                    value="<p>ç³»ç»Ÿå·²å°±ç»ª</p>",
                    label="ç³»ç»ŸçŠ¶æ€"
                )
            
            # å³ä¾§é¢æ¿
            with gr.Column(scale=2, elem_classes=["right-panel"]):
                # æ™ºèƒ½é—®ç­”
                gr.HTML('<h3>ğŸ’¬ æ™ºèƒ½é—®ç­”</h3>')
                
                # èŠå¤©è®°å½•
                chatbot = gr.Chatbot(
                    height=400,
                    show_label=False,
                    show_copy_button=True,
                    type="messages"
                )
                
                # è¾“å…¥åŒºåŸŸ
                with gr.Row():
                    msg_input = gr.Textbox(
                        placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        show_label=False,
                        scale=4
                    )
                    send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                
                # æ¸…ç©ºæŒ‰é’®
                clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")
        
        # å…¨å±€çŠ¶æ€ - ä½¿ç”¨Noneåˆå§‹åŒ–ï¼Œé¿å…æ·±æ‹·è´é—®é¢˜
        current_vectorstore = gr.State(None)
        current_conversation_store = gr.State(None)
        
        def update_project_stats(project_name):
            """æ›´æ–°é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯"""
            if not project_name:
                return "<p>è¯·é€‰æ‹©ä¸€ä¸ªé¡¹ç›®</p>"
            
            stats = rag_system.get_project_stats(project_name)
            if "error" in stats:
                return f"<p style='color: red;'>{stats['error']}</p>"
            
            return f"""
            <div class="project-stats">
                <p><strong>é¡¹ç›®æ–‡ä»¶:</strong> {', '.join(stats['files'])}</p>
                <p><strong>å‘é‡åŒ–çŠ¶æ€:</strong> <span style="color: {'green' if stats['vectorized'] else 'red'}">{stats['status']}</span></p>
                <p><strong>æ–‡ä»¶æ•°é‡:</strong> {stats['total_files']}</p>
            </div>
            """
        
        def process_project(project_name, current_vectorstore_state):
            """å¤„ç†é¡¹ç›®å‘é‡åŒ–"""
            if not project_name:
                return "è¯·å…ˆé€‰æ‹©ä¸€ä¸ªé¡¹ç›®", current_vectorstore_state
            
            try:
                # æ‰§è¡Œå‘é‡åŒ–å¤„ç†
                success = rag_system.extract_and_vectorize(project_name)
                
                if success:
                    # é‡æ–°åŠ è½½å‘é‡å­˜å‚¨
                    rag_system.vectorstore = rag_system._load_all_vectorized()
                    return f"âœ… é¡¹ç›® {project_name} å¤„ç†å®Œæˆï¼", "loaded"  # è¿”å›çŠ¶æ€æ ‡è¯†è€Œä¸æ˜¯å¯¹è±¡
                else:
                    return f"âŒ é¡¹ç›® {project_name} å¤„ç†å¤±è´¥", current_vectorstore_state
                    
            except Exception as e:
                return f"âŒ å¤„ç†å‡ºé”™: {str(e)}", current_vectorstore_state
        
        def chat_with_rag(message, history, vectorstore_state):
            """RAGèŠå¤©åŠŸèƒ½ - æµå¼è¾“å‡º"""
            if not message.strip():
                return history, ""
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•ï¼ˆä½¿ç”¨messagesæ ¼å¼ï¼‰
            history.append({"role": "user", "content": message})
            
            # å…ˆæ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            yield history, ""
            
            # è·å–AIå›ç­”ï¼ˆæµå¼ï¼‰
            try:
                # ç›´æ¥ä½¿ç”¨rag_systemçš„å‘é‡å­˜å‚¨ï¼Œä¸ä¾èµ–çŠ¶æ€
                response_generator = rag_system.query(message)
                
                # åˆå§‹åŒ–AIå›ç­”
                ai_response = ""
                
                # æµå¼æ›´æ–°å›ç­”
                for chunk in response_generator:
                    if chunk:
                        ai_response += chunk
                        # æ·»åŠ AIå›ç­”åˆ°å†å²è®°å½•
                        if len(history) > 0 and history[-1]["role"] == "user":
                            # å¦‚æœæœ€åä¸€æ¡æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ AIå›ç­”
                            history.append({"role": "assistant", "content": ai_response})
                        else:
                            # æ›´æ–°æœ€åä¸€æ¡AIæ¶ˆæ¯
                            history[-1] = {"role": "assistant", "content": ai_response}
                        yield history, ""
                
            except Exception as e:
                error_msg = f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
                history.append({"role": "assistant", "content": error_msg})
                yield history, ""
        
        def refresh_projects():
            """åˆ·æ–°é¡¹ç›®åˆ—è¡¨"""
            projects = rag_system.get_available_projects()
            return gr.Dropdown(choices=projects)
        
        # ç»‘å®šäº‹ä»¶
        project_dropdown.change(
            update_project_stats,
            inputs=[project_dropdown],
            outputs=[stats_display]
        )
        
        process_btn.click(
            process_project,
            inputs=[project_dropdown, current_vectorstore],
            outputs=[process_status, current_vectorstore]
        )
        
        send_btn.click(
            chat_with_rag,
            inputs=[msg_input, chatbot, current_vectorstore],
            outputs=[chatbot, msg_input]
        )
        
        msg_input.submit(
            chat_with_rag,
            inputs=[msg_input, chatbot, current_vectorstore],
            outputs=[chatbot, msg_input]
        )
        
        clear_btn.click(lambda: [], outputs=[chatbot])
        refresh_btn.click(refresh_projects, outputs=[project_dropdown])
    
    return demo

if __name__ == "__main__":
    # å¯åŠ¨ç•Œé¢
    demo = create_gradio_interface()
    demo.queue().launch(
        server_name='127.0.0.1',
        server_port=7861,
        share=False,
        debug=True
    )

